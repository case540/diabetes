{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"OMPI_MCA_opal_cuda_support\"] = \"true\"\n",
    "os.environ[\"CONDA_OVERRIDE_GLIBC\"] = \"2.56\"\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled and processed for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafceec4ed1d4254bb4673b6ac021c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def uniform_fixed_sampling(data, window_size, num_sections):\n",
    "    # Split data into sections\n",
    "    sections = np.array_split(data, num_sections)\n",
    "\n",
    "    # Initialize list to hold samples\n",
    "    samples = []\n",
    "\n",
    "    # For each position within a section\n",
    "    for pos in range(window_size):\n",
    "        # Sample the same position from each section\n",
    "        sample = [section[pos] for section in sections]\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples\n",
    "\n",
    "sample_path = \"/share/home/liangzhongming/930/CGMformer/data/Shanghai_finetune\"\n",
    "save_path = \"/share/home/liangzhongming/930/CGMformer/data/Shanghai_finetune_subSampleV3_2cls\"\n",
    "\n",
    "train_dataset=load_from_disk(sample_path)\n",
    "# Determine the number of sections based on the window size\n",
    "original_length = len(train_dataset['input_ids'][0])  # assuming all sequences have the same length\n",
    "window_size = 3\n",
    "num_sections = original_length // window_size\n",
    "\n",
    "# Initialize lists to hold the sampled data\n",
    "sampled_input_ids = []\n",
    "sampled_types = []      \n",
    "\n",
    "# Loop over the original data\n",
    "cnt = 0\n",
    "for input_ids, type_ in zip(train_dataset['input_ids'], train_dataset['types']):\n",
    "\n",
    "    # Sample the input_ids sequence\n",
    "    sampled_ids = uniform_fixed_sampling(input_ids, window_size, num_sections)\n",
    "    \n",
    "    # Repeat the type_ for the number of samples\n",
    "    sampled_type_ = [type_] * len(sampled_ids)\n",
    "    \n",
    "    # Append to the lists\n",
    "    sampled_input_ids.extend(sampled_ids)\n",
    "    sampled_types.extend(sampled_type_)\n",
    "\n",
    "# Create a new dataset from the sampled data\n",
    "sampled_dataset = Dataset.from_dict({\n",
    "    'input_ids': sampled_input_ids,\n",
    "    'types': sampled_types\n",
    "})\n",
    "\n",
    "# Save the new dataset\n",
    "sampled_dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 300, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "train_datsset_path = \"/share/home/liangzhongming/930/CGMformer/data/Shanghai_finetune_subSampleV3_2cls\"\n",
    "train_dataset=load_from_disk(train_datsset_path)\n",
    "train_dataset['types']\n",
    "target_names = set(list(Counter(train_dataset['types']).keys()))\n",
    "target_names\n",
    "\n",
    "cnt_0 = 0\n",
    "cnt_1 = 0\n",
    "cnt_2 = 0\n",
    "len(train_dataset['types'])\n",
    "for i in range(len(train_dataset['types'])):\n",
    "    if train_dataset['types'][i] == 0:\n",
    "        cnt_0 += 1\n",
    "    elif train_dataset['types'][i] == 1:\n",
    "        cnt_1 += 1\n",
    "    elif train_dataset['types'][i] == 2:\n",
    "        cnt_2 += 1\n",
    "cnt_0, cnt_1, cnt_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequent sequence sampling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'types', 'age', 'bmi', 'hba1c', 'homa-b', 'homa-is', 'index', 'Fast_s', 'Fast_e', 'Dawn_s', 'Dawn_e', 'Breakfast_s', 'Breakfast_e', 'Lunch_s', 'Lunch_e', 'Dinner_s', 'Dinner_e', 'input_ids'],\n",
       "     num_rows: 1981\n",
       " }),\n",
       " 1981,\n",
       " 144,\n",
       " 1981)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datsset_path = \"/share/home/liangzhongming/930/CGMformer/data/8_7_data/Shanghai_downsampled_144\"\n",
    "train_dataset=load_from_disk(train_datsset_path)\n",
    "train_dataset, len(train_dataset['input_ids']), len(train_dataset['input_ids'][0]), len(train_dataset['types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'types'],\n",
       "     num_rows: 13480\n",
       " }),\n",
       " 13480,\n",
       " 96,\n",
       " 13480,\n",
       " 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datsset_path = \"/share/home/liangzhongming/930/CGMformer/data/Shanghai_train_subSample\"\n",
    "train_dataset=load_from_disk(train_datsset_path)\n",
    "train_dataset, len(train_dataset['input_ids']), len(train_dataset['input_ids'][0]), len(train_dataset['types']), train_dataset['types'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def sliding_window_sampling(data, window_size, step_size):\n",
    "    samples = []\n",
    "    for i in range(0, len(data) - window_size + 1, step_size):\n",
    "        sample = data[i:i + window_size]\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "# Determine step size based on the number of required samples\n",
    "original_length = len(train_dataset['input_ids'][0])  # assuming all sequences have the same length\n",
    "num_samples = 10\n",
    "window_size = 96\n",
    "step_size = (original_length - window_size) // (num_samples - 1)\n",
    "\n",
    "# Initialize lists to hold the sampled data\n",
    "sampled_input_ids = []\n",
    "sampled_types = []\n",
    "\n",
    "# Loop over the original data\n",
    "for input_ids, type_ in zip(train_dataset['input_ids'], train_dataset['types']):\n",
    "    # Sample the input_ids sequence\n",
    "    sampled_ids = sliding_window_sampling(input_ids, window_size, step_size)\n",
    "    \n",
    "    # Repeat the type_ for the number of samples\n",
    "    sampled_type_ = [type_] * len(sampled_ids)\n",
    "    \n",
    "    # Append to the lists\n",
    "    sampled_input_ids.extend(sampled_ids)\n",
    "    sampled_types.extend(sampled_type_)\n",
    "\n",
    "# Create a new dataset from the sampled data\n",
    "sampled_dataset = Dataset.from_dict({\n",
    "    'input_ids': sampled_input_ids,\n",
    "    'types': sampled_types\n",
    "})\n",
    "\n",
    "# Save the new dataset\n",
    "sampled_dataset.save_to_disk(\"/share/home/liangzhongming/930/CGMformer/data/Shanghai_train_subSample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/home/liangzhongming/anaconda3/envs/Geneformer/lib/python3.8/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 96, using nperseg = 96\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS test result for means: KstestResult(statistic=0.003214638971315529, pvalue=1.0, statistic_location=103.33749999999999, statistic_sign=1)\n",
      "KS test result for vars: KstestResult(statistic=0.004698318496538081, pvalue=1.0, statistic_location=450.92937500000005, statistic_sign=1)\n",
      "KS test result for ACFs: KstestResult(statistic=0.27097515564089136, pvalue=0.0, statistic_location=0.1796633494112581, statistic_sign=-1)\n",
      "KS test result for PSDs: KstestResult(statistic=0.31095972306526637, pvalue=0.0, statistic_location=17.56655374127823, statistic_sign=1)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from statsmodels.tsa.stattools import acf, adfuller\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "sample_path = \"/share/home/liangzhongming/930/CGMformer/data/Shanghai_train\"\n",
    "save_path = \"/share/home/liangzhongming/930/CGMformer/data/Shanghai_train_subSampleV3\"\n",
    "\n",
    "train_dataset = load_from_disk(sample_path)\n",
    "sampled_dataset = load_from_disk(save_path)\n",
    "\n",
    "# Calculate statistics for original data\n",
    "original_means = [np.mean(seq) for seq in train_dataset['input_ids']]\n",
    "original_vars = [np.var(seq) for seq in train_dataset['input_ids']]\n",
    "original_acfs = [acf(seq, nlags=50) for seq in train_dataset['input_ids']]\n",
    "original_p_values = [adfuller(seq)[1] for seq in train_dataset['input_ids']]\n",
    "original_psds = [welch(seq)[1] for seq in train_dataset['input_ids']]  # select PSDs\n",
    "\n",
    "# Calculate statistics for sampled data\n",
    "sampled_means = [np.mean(seq) for seq in sampled_dataset['input_ids']]\n",
    "sampled_vars = [np.var(seq) for seq in sampled_dataset['input_ids']]\n",
    "sampled_acfs = [acf(seq, nlags=50) for seq in sampled_dataset['input_ids']]\n",
    "sampled_p_values = [adfuller(seq)[1] for seq in sampled_dataset['input_ids']]\n",
    "sampled_psds = [welch(seq)[1] for seq in sampled_dataset['input_ids']]  # select PSDs\n",
    "\n",
    "# Compare distributions of means, vars, acfs and psds using Kolmogorov-Smirnov test\n",
    "ks_result_means = ks_2samp(original_means, sampled_means)\n",
    "ks_result_vars = ks_2samp(original_vars, sampled_vars)\n",
    "ks_result_acfs = ks_2samp(np.concatenate(original_acfs), np.concatenate(sampled_acfs))\n",
    "ks_result_psds = ks_2samp(np.concatenate(original_psds), np.concatenate(sampled_psds))\n",
    "\n",
    "# Print KS test results\n",
    "print(\"KS test result for means:\", ks_result_means)\n",
    "print(\"KS test result for vars:\", ks_result_vars)\n",
    "print(\"KS test result for ACFs:\", ks_result_acfs)\n",
    "print(\"KS test result for PSDs:\", ks_result_psds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geneformer",
   "language": "python",
   "name": "geneformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
